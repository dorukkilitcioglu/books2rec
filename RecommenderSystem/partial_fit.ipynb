{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom libraries\n",
    "sys.path.append('../Util')\n",
    "from loader import get_books, get_book_dataframe, get_book_features\n",
    "from joiner import get_ratings, get_joint, load_amazon, load_goodreads\n",
    "from reduction import reduce_matrix, get_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_to_features(p, features):\n",
    "    p_sparse = scipy.sparse.csr_matrix(p)\n",
    "    # map new user to concept space by p*features\n",
    "    user_to_concept = p_sparse.dot(features)\n",
    "    # map user back to itme space with user_to_concept * featuresT\n",
    "    result = user_to_concept.dot(features.T).todense()\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(p, q, user_bias, item_bias, global_bias):\n",
    "    pred_ratings = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        pred = global_bias + user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "        # pred = global_bias + user_bias + np.dot(p, q[i])\n",
    "        pred_ratings[i] = pred\n",
    "    return pred_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recs(result, books, n, q):\n",
    "    recs = []\n",
    "    for i in range(len(result)):\n",
    "        if q[i] == 0: # book user hasn't already rated\n",
    "            recs.append((i, result[i]))\n",
    "        else:\n",
    "            recs.append((i, float('-inf'))) \n",
    "            # recs.append((i, result[i])) #leave this to verify things actually working\n",
    "    recs = sorted(recs, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    top_titles = []\n",
    "    for i in range(n):\n",
    "        book_id = recs[i][0]\n",
    "        title = books.iloc[book_id]['title']\n",
    "        top_titles.append(title)\n",
    "    return top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to where you save and load all data\n",
    "data_path = '../../goodbooks-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found books_dataframe in file...\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe from books\n",
    "books = get_book_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Hunger Games (The Hunger Games, #1)'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.iloc[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cu2rec components\n",
    "filename = '../.tmp/goodbooks_sorted_f300'\n",
    "q = genfromtxt('{}_q.csv'.format(filename), delimiter=',')\n",
    "item_bias = genfromtxt('{}_item_bias.csv'.format(filename), delimiter=',')\n",
    "\n",
    "\n",
    "# surprise components\n",
    "# filename = '../.tmp/svd_100_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/Q_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/item_bias_300.npy'\n",
    "# item_bias = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert global bias to float - get from whatever dataset you used\n",
    "global_bias = 3.919866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user from goodreads\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_mystery_scifi_hates_fantasy.npz')\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_fantasy.npz')\n",
    "sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_nickgreenquist.npz')\n",
    "new_user_ratings_scaled = sparse_new_user_scaled.toarray()\n",
    "new_user_ratings_scaled = np.array(new_user_ratings_scaled[0].tolist())\n",
    "new_user_ratings = np.copy(new_user_ratings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undo the rating mapping we usually do\n",
    "\n",
    "# Turn 1-5 rating scale into negative - positive scale\n",
    "# original mapper: ratings_mapper = {0:0, 1:-2, 2:-1, 3:1, 4:2, 5:3}\n",
    "ratings_mapper = {0:0, -2:-1, -1:-2, 1:3, 2:4, 3:5}\n",
    "for i in range(len(q)):\n",
    "    new_user_ratings[i] = ratings_mapper[new_user_ratings_scaled[i]]\n",
    "new_user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array of indices of books this user has actually rated\n",
    "indices = []\n",
    "for i in range(len(new_user_ratings)):\n",
    "    if new_user_ratings[i] != 0:\n",
    "        indices.append(i)\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "learning_rate = 0.07\n",
    "user_bias_reg = 0.0\n",
    "P_reg = 0.0\n",
    "\n",
    "# updates per rated book\n",
    "iterations = 5\n",
    "\n",
    " # how many iterations to see the total loss at this step - remove in webapp!\n",
    "calculate_total_loss = 1\n",
    "\n",
    "n_factors = q.shape[1]\n",
    "cols = q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the user_bias for this user\n",
    "new_user_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. set up new random P\n",
    "mu, sigma = 0, 0.1\n",
    "p = np.random.normal(mu, (sigma / n_factors), n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE at Iteration 0: 1.770656185694843\n",
      "RMSE at Iteration 1: 1.521928105467638\n",
      "RMSE at Iteration 2: 1.4149172616138068\n",
      "RMSE at Iteration 3: 1.3436697781979676\n",
      "RMSE at Iteration 4: 1.288625839338802\n"
     ]
    }
   ],
   "source": [
    "# 3. computer small number of iterations of SGD\n",
    "for iteration in range(iterations):\n",
    "    \n",
    "    #= periodically calculate total loss and output\n",
    "    if iteration == 0 or iteration == iterations - 1 or iteration % calculate_total_loss == 0:\n",
    "        total_loss = 0.0\n",
    "        for i in indices:\n",
    "            rating = new_user_ratings[i]\n",
    "            pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "            # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "            error = rating - pred\n",
    "            total_loss += pow(error, 2)\n",
    "            \n",
    "        last_rmse = rmse\n",
    "        rmse = math.sqrt(total_loss / len(indices))\n",
    "        print(\"RMSE at Iteration {}: {}\".format(iteration, rmse))\n",
    "        \n",
    "        # Update learning rate if needed\n",
    "        if last_rmse < rmse:\n",
    "            current_patience -=1\n",
    "        if current_patience <= 0:\n",
    "            current_patience = max_patience\n",
    "            learning_rate *= learning_rate_decay\n",
    "\n",
    "            print(\"New Learning Rate: {}\", learning_rate)\n",
    "\n",
    "    # Gradient Descent using every book - ucomment below to go back to SGD\n",
    "    # i = random.choice(indices)\n",
    "    for i in indices:\n",
    "    \n",
    "        # calculate loss on random item\n",
    "        rating = new_user_ratings[i]\n",
    "        pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "        # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "        error = rating - pred\n",
    "\n",
    "        # update P\n",
    "        for f in range(n_factors):\n",
    "            p_update = learning_rate * (error * q[i][f] - P_reg * p[f])\n",
    "            p[f] += p_update\n",
    "\n",
    "        # update user bias\n",
    "        ub_update = learning_rate * (error - user_bias_reg * new_user_bias)\n",
    "        new_user_bias += ub_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using partial fit\n",
    "predictions_partial_fit = get_predictions(p, q, new_user_bias, item_bias, global_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Road to Serfdom\n",
      "Feeling Good: The New Mood Therapy\n",
      "The Final Empire (Mistborn, #1)\n",
      "The Heretic Queen\n",
      "The Luxe (Luxe, #1)\n",
      "Hope: A Memoir of Survival in Cleveland\n",
      "The Blade Itself (The First Law, #1)\n",
      "No One Here Gets Out Alive\n",
      "The Raw Shark Texts\n",
      "The Troop\n",
      "Laskar Pelangi (Tetralogi Laskar Pelangi, #1)\n",
      "Not Without My Daughter\n",
      "The Mortal Instruments Boxed Set: City of Bones; City of Ashes; City of Glass (The Mortal Instruments, #1-3)\n",
      "The Meaning of Marriage: Facing the Complexities of Commitment with the Wisdom of God\n",
      "The Scent of Rain and Lightning\n",
      "Falling into Place\n",
      "Night World, No. 2 (Night World, #4-6)\n",
      "Any Human Heart\n",
      "The Sculptor\n",
      "Lord of Scoundrels (Scoundrels, #3)\n",
      "The King of Attolia (The Queen's Thief, #3)\n",
      "The Queen of the Damned (The Vampire Chronicles, #3)\n",
      "Beautiful Creatures (Caster Chronicles, #1)\n",
      "Gone Girl\n",
      "Trinity\n"
     ]
    }
   ],
   "source": [
    "recs_partial_fit = get_top_n_recs(predictions_partial_fit, books, 25, new_user_ratings)\n",
    "for rec in recs_partial_fit:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCombine recs from partial fit with recs from mapping to feature matrix using log_rank\\n\\n'"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Combine recs from partial fit with recs from mapping to feature matrix using log_rank\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_matrix exists in file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 82203)"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce feature matrix\n",
    "feature_matrix = get_book_features(books)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using feature matrix\n",
    "predictions_features = map_user_to_features(new_user_ratings, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLog Ranking\\n'"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Log Ranking\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuple of book_id and rating for each method, then sort\n",
    "partial_fit_ratings = []\n",
    "feature_ratings = []\n",
    "for i in range(len(books)):\n",
    "    partial_fit_ratings.append((i, predictions_partial_fit[i]))\n",
    "    feature_ratings.append((i, predictions_features[i]))\n",
    "\n",
    "partial_fit_ratings = sorted(partial_fit_ratings, key=lambda x: x[1], reverse=True)\n",
    "feature_ratings = sorted(feature_ratings, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map book_id to the rank for each method\n",
    "id_to_rank_partial_fit = {}\n",
    "id_to_rank_features = {}\n",
    "for i in range(len(books)):\n",
    "    book_id = partial_fit_ratings[i][0]\n",
    "    id_to_rank_partial_fit[book_id] = math.log(i+1)\n",
    "\n",
    "    book_id = feature_ratings[i][0]\n",
    "    id_to_rank_features[book_id] = math.log(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9798\n"
     ]
    }
   ],
   "source": [
    "weight_feature = 0.5\n",
    "\n",
    "rankings = []\n",
    "for i in range(len(books)):\n",
    "    if new_user_ratings[i] == 0:\n",
    "        rank = weight_feature*id_to_rank_features[i] + (1.0-weight_feature)*id_to_rank_partial_fit[i]\n",
    "        rankings.append((rank, i))\n",
    "rankings = sorted(rankings, key=lambda x: x[0])\n",
    "print(len(rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_books = []\n",
    "for i in range(25):\n",
    "    book_id = rankings[i][1]\n",
    "    book = books.iloc[book_id] # index is book_id - 1\n",
    "    book['rank'] = i + 1\n",
    "    top_books.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Raw Shark Texts\n",
      "The Final Empire (Mistborn, #1)\n",
      "The Book of Three (The Chronicles of Prydain, #1)\n",
      "A Wrinkle in Time (A Wrinkle in Time Quintet, #1)\n",
      "Un Lun Dun\n",
      "The King of Attolia (The Queen's Thief, #3)\n",
      "J.R.R. Tolkien 4-Book Boxed Set: The Hobbit and The Lord of the Rings\n",
      "The Road to Serfdom\n",
      "The Blade Itself (The First Law, #1)\n",
      "The Call of the Wild\n",
      "Library of Souls (Miss Peregrine's Peculiar Children, #3)\n",
      "The Lost World (Professor Challenger, #1)\n",
      "Dandelion Wine (Green Town, #1)\n",
      "Feeling Good: The New Mood Therapy\n",
      "My Ãntonia\n",
      "Rebecca\n",
      "The Prestige\n",
      "The Luxe (Luxe, #1)\n",
      "Incarceron (Incarceron, #1)\n",
      "Hope: A Memoir of Survival in Cleveland\n",
      "His Dark Materials (His Dark Materials #1-3)\n",
      "Palace of Stone (Princess Academy, #2)\n",
      "Tarzan of the Apes (Tarzan, #1)\n",
      "I Capture the Castle\n",
      "The Heretic Queen\n"
     ]
    }
   ],
   "source": [
    "for book in top_books:\n",
    "    print(book['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
